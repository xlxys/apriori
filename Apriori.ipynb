{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apriori algorithm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n",
    "1. load data from csv file\n",
    "2. convert data to binary data\n",
    "3. convert data to list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    return data\n",
    "\n",
    "def process(data):\n",
    "\n",
    "    # convert data to numpy array\n",
    "    data = data.to_numpy()\n",
    "\n",
    "    return data\n",
    "\n",
    "# def process_noneBianry(data):\n",
    "\n",
    "#     # Split nominal and numerical data\n",
    "#     nominal_data = data.select_dtypes(include=['object'])\n",
    "#     numerical_data = data.select_dtypes(include=['int', 'float'])\n",
    "    \n",
    "#     # One-hot encode nominal data\n",
    "#     nominal_data = pd.get_dummies(nominal_data)\n",
    "\n",
    "#     # Discretize numerical data\n",
    "#     discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "#     discretized_data = pd.DataFrame(discretizer.fit_transform(numerical_data), columns=numerical_data.columns)\n",
    "\n",
    "#     # Concatenate nominal and numerical data\n",
    "#     processed_data = pd.concat([nominal_data, discretized_data], axis=1)\n",
    "\n",
    "#     # Convert data to binary format\n",
    "#     processed_data = np.where(processed_data > 0, 1, 0)\n",
    "\n",
    "#     # Save processed data to file\n",
    "#     np.savetxt(\"processed_data.csv\", processed_data, delimiter=\",\", fmt='%d')\n",
    "\n",
    "#     return processed_data\n",
    "\n",
    "\n",
    "def data_to_transactions(data):\n",
    "    \n",
    "    transactions = []\n",
    "    for row in data:\n",
    "        row = np.where(row > 0)[0]\n",
    "        transactions.append(row)\n",
    "\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"ex.csv\")\n",
    "\n",
    "# data = load_data(\"basket_analysis.csv\")\n",
    "\n",
    "variables = data.columns\n",
    "\n",
    "data = process(data)\n",
    "\n",
    "data = data_to_transactions(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apriori algorithm\n",
    "\n",
    "1. generate candidate itemsets\n",
    "2. generate frequent itemsets\n",
    "3. generate association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_itemsets(frequent_itemsets, k):\n",
    "    \n",
    "    if k == 2:\n",
    "        candidate_itemsets = []\n",
    "        for i in range(len(frequent_itemsets)):\n",
    "            for j in range(i+1, len(frequent_itemsets)):\n",
    "                itemset_1 = [frequent_itemsets[i]]\n",
    "                # print (itemset_1)\n",
    "                itemset_2 = [frequent_itemsets[j]]\n",
    "                # print (itemset_2)\n",
    "                if itemset_1[:k-2] == itemset_2[:k-2]:\n",
    "                    candidate_itemset = itemset_1 + [itemset_2[-1]]\n",
    "                    candidate_itemset.sort()\n",
    "                    candidate_itemsets.append(set(candidate_itemset))\n",
    "                \n",
    "    else:\n",
    "        candidate_itemsets = []\n",
    "        for i in range(len(frequent_itemsets)):\n",
    "            for j in range(i+1, len(frequent_itemsets)):\n",
    "                itemset_1 = frequent_itemsets[i]\n",
    "                itemset_2 = frequent_itemsets[j]\n",
    "                candidate_itemset = itemset_1.union(itemset_2)\n",
    "                if len(candidate_itemset) == k and candidate_itemset not in candidate_itemsets:\n",
    "                    candidate_itemsets.append(candidate_itemset)\n",
    "\n",
    "    return candidate_itemsets\n",
    "\n",
    "def get_frequency(transactions):\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if item in item_counts:\n",
    "                item_counts[item] += 1\n",
    "            else:\n",
    "                item_counts[item] = 1\n",
    "    \n",
    "    return item_counts\n",
    "\n",
    "\n",
    "def closed(itemsets, frequency):\n",
    "    closed_itemsets = []\n",
    "    leng = 0\n",
    "    for itemset in range(len(itemsets)):\n",
    "        itemsetlen = len(itemsets[itemset])\n",
    "        print(itemsetlen)\n",
    "        leng += itemsetlen\n",
    "        print(leng)\n",
    "        \n",
    "\n",
    "        for item in range (len(itemsets[itemset])):\n",
    "            \n",
    "            equ = True\n",
    "            # print(itemsets[itemset][item])\n",
    "\n",
    "            if itemset+1 < len(itemsets):\n",
    "                # print(\"item\",item)    \n",
    "                for supersets in range(len(itemsets[itemset+1])):\n",
    "                    # print(\"super\",supersets)  \n",
    "                    # print(itemsets[itemset+1][supersets])\n",
    "                    if isinstance(itemsets[itemset][item], set):\n",
    "                        if itemsets[itemset][item].issubset(itemsets[itemset+1][supersets]):\n",
    "                            \n",
    "                            if frequency[item] == frequency[leng+supersets] :\n",
    "                                if itemsets[itemset][item] == {0,2}:\n",
    "                                    print(\"item\",item) \n",
    "                                    \n",
    "                                equ = False\n",
    "                                \n",
    "                                break\n",
    "\n",
    "                    else:\n",
    "                        if itemsets[itemset][item] in itemsets[itemset+1][supersets]:\n",
    "  \n",
    "                            if frequency[item] == frequency[leng+supersets] :\n",
    "                             \n",
    "                                equ = False\n",
    "                                break\n",
    "                            \n",
    "                if equ == True and itemsets[itemset][item] not in closed_itemsets:\n",
    "                    closed_itemsets.append(itemsets[itemset][item])\n",
    "\n",
    "    return closed_itemsets\n",
    "\n",
    "\n",
    "def association_rules(itemsets):\n",
    "    association_rules = []\n",
    "    for itemset in itemsets:\n",
    "        for i in range(len(itemset)):\n",
    "            # generate all possible left-hand side (LHS) of the association rule\n",
    "            for lhs in itertools.combinations(itemset, i):\n",
    "                lhs = set(lhs)\n",
    "                # generate the right-hand side (RHS) of the association rule\n",
    "                rhs = itemset - lhs\n",
    "                if lhs and rhs:\n",
    "                    association_rules.append((lhs, rhs))\n",
    "    return association_rules\n",
    "\n",
    "\n",
    "def confidence(itemsets, transactions, min_confidence):\n",
    "    rules = association_rules(itemsets)\n",
    "    confidence_rules = []\n",
    "    for rule in rules:\n",
    "        lhs = rule[0]\n",
    "        rhs = rule[1]\n",
    "        lhs_count = 0\n",
    "        rhs_count = 0\n",
    "        for transaction in transactions:\n",
    "            if lhs.issubset(transaction):\n",
    "                lhs_count += 1\n",
    "                if rhs.issubset(transaction):\n",
    "                    rhs_count += 1\n",
    "        confidence = rhs_count / lhs_count\n",
    "        if confidence >= min_confidence:\n",
    "            confidence_rules.append((lhs, rhs, confidence))\n",
    "    return confidence_rules\n",
    "\n",
    "\n",
    "def flatten_list(lst):\n",
    "    flattened = []\n",
    "    for element in lst:\n",
    "        if isinstance(element, list):\n",
    "            flattened.extend(flatten_list(element))\n",
    "        else:\n",
    "            flattened.append(element)\n",
    "    return flattened\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(transactions, min_support):\n",
    "    # Counting the frequency of each item in the dataset\n",
    "    item_counts = get_frequency(transactions)\n",
    "\n",
    "    # Removing infrequent items from the item_counts dictionary\n",
    "    item_counts = {k: v for k, v in item_counts.items() if v >= min_support}\n",
    "\n",
    "    # Extracting frequent items and their counts\n",
    "    frequent_items = list(item_counts.keys())\n",
    "    frequent_items_counts = list(item_counts.values())\n",
    "\n",
    "    \n",
    "    # Finding frequent itemsets\n",
    "    frequent_itemsets = [frequent_items]\n",
    "    # print (frequent_itemsets) \n",
    "\n",
    "    k = 2\n",
    "    while True:\n",
    "        # Generating candidate itemsets\n",
    "        \n",
    "        # print(frequent_itemsets[-1])\n",
    "        candidate_itemsets = generate_candidate_itemsets(frequent_itemsets[-1], k)\n",
    "        # print(candidate_itemsets)\n",
    "\n",
    "        # Counting the frequency of each candidate itemset in the dataset\n",
    "        candidate_itemsets_counts = [0] * len(candidate_itemsets)\n",
    "        for transaction in transactions:\n",
    "            for i, itemset in enumerate(candidate_itemsets):\n",
    "                if set(itemset).issubset(set(transaction)):\n",
    "                    candidate_itemsets_counts[i] += 1\n",
    "        # print(candidate_itemsets_counts)\n",
    "\n",
    "        # Removing infrequent candidate itemsets\n",
    "        frequent_itemsets_k = []\n",
    "        frequent_itemsets_counts_k = []\n",
    "        for i, itemset in enumerate(candidate_itemsets):\n",
    "            if candidate_itemsets_counts[i] >= min_support:\n",
    "                frequent_itemsets_k.append(itemset)\n",
    "                frequent_itemsets_counts_k.append(candidate_itemsets_counts[i])\n",
    "        \n",
    "        # print(frequent_itemsets_k)\n",
    "        # Stopping if no frequent itemsets were found\n",
    "        if frequent_itemsets_k == []:\n",
    "            break\n",
    "        \n",
    "        # Adding frequent itemsets to the list\n",
    "        frequent_itemsets.append(frequent_itemsets_k)\n",
    "        frequent_items_counts.append(frequent_itemsets_counts_k)\n",
    "        # print (frequent_itemsets)\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    # Flattening the list of frequent itemsets and their counts\n",
    "    frequent_items_counts = flatten_list(frequent_items_counts)\n",
    "\n",
    "    close = closed(frequent_itemsets, frequent_items_counts)\n",
    "\n",
    "    rules = association_rules(flatten_list(frequent_itemsets[1:]))\n",
    "\n",
    "    print(\"rules\", rules)\n",
    "\n",
    "    # confi = confidence(flatten_list(frequent_itemsets[1:]), transactions, min_confidence)\n",
    "\n",
    "   \n",
    "    return frequent_itemsets, frequent_items_counts, close, rules\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "4\n",
      "9\n",
      "1\n",
      "10\n",
      "rules [({0}, {1}), ({1}, {0}), ({0}, {2}), ({2}, {0}), ({1}, {2}), ({2}, {1}), ({2}, {5}), ({5}, {2}), ({0}, {1, 2}), ({1}, {0, 2}), ({2}, {0, 1}), ({0, 1}, {2}), ({0, 2}, {1}), ({1, 2}, {0})]\n",
      "Frequent itemsets: [[0, 1, 2, 3, 5], [{0, 1}, {0, 2}, {1, 2}, {2, 5}], [{0, 1, 2}]]\n",
      "Frequent itemsets counts: [4, 4, 4, 3, 3, 4, 3, 3, 3, 3]\n",
      "Closed itemsets: [2, 3, {0, 1}, {0, 2}, {1, 2}, {2, 5}]\n"
     ]
    }
   ],
   "source": [
    "min_support = 3\n",
    "# min_confidence = 1\n",
    "\n",
    "frequent_itemsets, frequent_itemsets_counts, closed, rules = apriori(data, min_support)\n",
    "\n",
    "print(\"Frequent itemsets:\", frequent_itemsets)\n",
    "print(\"Frequent itemsets counts:\", frequent_itemsets_counts)\n",
    "print(\"Closed itemsets:\", closed)\n",
    "\n",
    "# print (association_rule(frequent_itemsets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
